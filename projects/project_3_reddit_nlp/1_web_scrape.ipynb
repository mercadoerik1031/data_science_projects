{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "The objective of this project is to build a machine learning model capable of accurately classifying Reddit posts into two categories: posts from the r/movies subreddit and posts from the r/books subreddit. The model will assist in automating the process of categorizing posts, potentially improving the user experience by providing targeted content and aiding in content moderation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import praw\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikmercado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/erikmercado/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This section of the notebook is dedicated to the data collection process. Using Reddit's API, I fetch posts from the `movies` and `books` subreddits across various categories such as 'hot', 'new', 'top', 'rising', and 'controversial'. The function `fetch_posts` is designed to equally balance the number of posts from each category to ensure diverse representation in my dataset.\n",
    "\n",
    "The posts are fetched and then concatenated, title and selftext together, to form a comprehensive text document that represents each post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=None,\n",
    "                     client_secret=None,\n",
    "                     user_agent=None,\n",
    "                     username=None,\n",
    "                     password=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_posts(subreddit_name, limit=100):\n",
    "    \"\"\"\n",
    "    Fetch an equal number of posts from different categories within a subreddit.\n",
    "    \n",
    "    :param subreddit_name: The name of the subreddit from which to fetch posts.\n",
    "    :param limit: The total maximum number of posts to fetch across all categories.\n",
    "    :return: A list of posts.\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    categories = ['hot', 'new', 'top', 'rising', 'controversial']\n",
    "    posts = []\n",
    "    limit_per_category = limit // len(categories)\n",
    "    \n",
    "    try:\n",
    "        for category in categories:\n",
    "            submissions = getattr(subreddit, category)(limit=limit_per_category)\n",
    "            for submission in submissions:\n",
    "                posts.append(submission.title + \" \" + submission.selftext)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is a critical step in natural language processing (NLP) tasks. The function `preprocess_text` performs several standard preprocessing steps on the raw text data:\n",
    "\n",
    "1. Converting all text to lowercase to ensure uniformity.\n",
    "2. Removing URLs, special characters, and numbers to focus on the textual content.\n",
    "3. Eliminating stopwords that do not contribute to the overall meaning.\n",
    "4. Lemmatizing the words, which involves converting each word to its base or dictionary form.\n",
    "\n",
    "These preprocessing steps are designed to clean and standardize the text data, thereby making it more suitable for modeling and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by removing URLs, special characters, numbers, and stopwords, and by performing lemmatization.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.loIr()  # Convert to loIrcase\n",
    "    text = re.sub(r'\\w+:\\/\\/\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, flags=re.MULTILINE)  # Remove special characters and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove excessive whitespace\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and POS Tagging\n",
    "\n",
    "After preprocessing the text, the next step involves tokenization and part-of-speech (POS) tagging using the function `tokenize_and_tag`. Tokenization breaks down the text into individual words or tokens. POS tagging assigns grammatical categories to each token, such as noun, verb, adjective, etc.\n",
    "\n",
    "POS tags provide additional contextual information that may be useful for distinguishing betIen the language used in `movies` and `books` posts. For instance, I might expect more past tense verbs in `books` discussions relating to narratives and storylines.\n",
    "\n",
    "By combining preprocessed text with POS tags, I create a rich set of features that could improve the performance of my classification model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_tag(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove punctuation from tokens\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Combine tokens and tags, exclude punctuation tags\n",
    "    tagged_tokens_str = [\"{}_{}\".format(token.loIr(), tag) for token, tag in tagged_tokens if tag not in string.punctuation]\n",
    "    return ' '.join(tagged_tokens_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Creation and Export\n",
    "\n",
    "With my text data preprocessed and POS tags generated, I compile my dataset into a Pandas DataFrame. This DataFrame includes the preprocessed text, POS tags, a combination of text and POS tags, and the corresponding labels (`0` for movies and `1` for books).\n",
    "\n",
    "The final step in the data preparation process is to save this DataFrame to a CSV file. This file will serve as an input for the exploratory data analysis (EDA) and modeling stages of my project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1624 posts from movies and 1323 posts from books\n"
     ]
    }
   ],
   "source": [
    "movies_posts = fetch_posts('movies', limit=2000)\n",
    "books_posts = fetch_posts('books', limit=2000)\n",
    "print(f\"Fetched {len(movies_posts)} posts from movies and {len(books_posts)} posts from books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to movies_books_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = [preprocess_text(text) for text in movies_posts + books_posts]\n",
    "\n",
    "# Generate POS tags for the original (not preprocessed) text\n",
    "pos_tags = [tokenize_and_tag(text) for text in movies_posts + books_posts]\n",
    "\n",
    "# Combine preprocessed text with POS tags\n",
    "text_and_pos = [text + \" \" + pos for text, pos in zip(preprocessed_text, pos_tags)]\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'text': preprocessed_text,\n",
    "    'pos': pos_tags,\n",
    "    'text_and_pos': text_and_pos, \n",
    "    'label': [0]*len(movies_posts) + [1]*len(books_posts)\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "data.to_csv('movies_books_dataset.csv', index=False)\n",
    "print(\"Dataset saved to movies_books_dataset.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, I successfully collected, preprocessed, and saved a dataset of Reddit posts from the `movies` and `books` subreddits. I have prepared the ground for the following stages, which will involve exploring this dataset to gain insights and developing a classification model that can accurately categorize new posts.\n",
    "\n",
    "my dataset is balanced and includes a combination of raw text, cleaned text, and POS tagged text, providing a robust starting point for building and training my machine learning models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
